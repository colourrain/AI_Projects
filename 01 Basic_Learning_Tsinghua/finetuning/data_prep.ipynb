{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96727cd0-3da5-48e1-a7db-4ac302e8b125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10adead9-e8b0-464d-86c9-326efc40eaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_datasets = load_dataset(\"opencsg/chinese-fineweb-edu\", split = \"train\", streaming=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad525826-c22c-462b-9bae-f72fd2f9501c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained dataset:\n",
      "{'text': '什么是三羊预防疫苗?\\n标准规模的绵羊养殖场接种程序,摇动或皮下或肌内注射接种80日龄,5毫升成年绵羊,3毫升6个月大的绵羊,4毫升杀死绵羊每6个月注射一次,高发病率每4个月皮下或肌肉注射5毫升成年绵羊和3毫升绵羊羔在6个月内用大肠杆菌灭活.\\n绵羊的疫苗接种是预防和控制绵羊感染的重要工具.\\n它用于预防羊炭疽.\\n它用于预防羔羊的快速疾病,痔疮,肠毒素血症和痔疮.\\n通常用于接种疫苗和通常用于接种疫苗的绵羊的绵羊数量,时间和数量.\\n接种期是绵羊出生前一个月,羔羊被切断前一个月,或者是产羊的时间.免疫接种一个月后,接种期为一年.\\n如何在春季接种疫苗,如何在春季接种疫苗.\\n第二,绵羊的四个共同种子或绵羊的四个共同种子四个快速绵羊的共同种子是瘟疫绵羊,鼠疫,肠毒素,羔羊痢疾,五莲苗是瘟疫的鼠瘟,鼠疫,肠毒素,羊痢疾,黑色疫苗.\\n接种期是绵羊出生前一个月和绵羊肥育和阉割前一个月.\\n绵羊疫苗接种计划和疫苗接种方法3.\\n对绵羊卵的疫苗接种:2个月大,皮内注射,1个部分进入脊的头部(0).\\n口蹄疫疫苗:3个月后在颈部肌肉注射1毫升.\\n一般而言,绵羊养殖场将为绵羊开展一系列反流行病学过程.在生产管理中,员工可以根据规划程序主要提供羊群防疫.\\n养羊的流行过程的秘密内容:一是防止成年羊流行的一个方案.\\n用于绵羊和绵羊的天花疫苗注意哪些绵羊应该注意接受绵羊天花疫苗接种绵羊天花疫苗我应该吗?\\n问题1需要注意严格遵守疫苗的储存方法,液体冷藏疫苗的储存以及粉末疫苗的冷冻保存.2,操作过程中应仔细阅读,空瓶疫苗,针头,针头,慎用.\\n牛群必须在春季浓缩,应该用绵羊疫苗接种程序在春季解冻绵羊.应该利用绵羊的免疫系统在春季解冻绵羊和绵羊的免疫系统.\\n目前,我区羊寄生虫主要包括蚜虫,蚜虫,肠胃线虫,肝脏皮屑和球虫.\\n有许多类型的常用驱虫药,如左手除草,消除了这么多的线虫.'}\n"
     ]
    }
   ],
   "source": [
    "n = 1\n",
    "print(\"Pretrained dataset:\")\n",
    "top_n = itertools.islice(pretrained_datasets,n)\n",
    "for i in top_n:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c4805eb-d27d-4655-872d-3d42a4251be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer'],\n",
      "    num_rows: 1400\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "instruction_dataset_df = load_dataset(\"kotzeje/lamini_docs.jsonl\", split = \"train\", trust_remote_code=True)\n",
    "print(instruction_dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5ca05d0-0001-4e39-bfd5-471a3ab1ea4a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained dataset:\n",
      "{'question': 'How can I evaluate the performance and quality of the generated text from Lamini models?', 'answer': \"There are several metrics that can be used to evaluate the performance and quality of generated text from Lamini models, including perplexity, BLEU score, and human evaluation. Perplexity measures how well the model predicts the next word in a sequence, while BLEU score measures the similarity between the generated text and a reference text. Human evaluation involves having human judges rate the quality of the generated text based on factors such as coherence, fluency, and relevance. It is recommended to use a combination of these metrics for a comprehensive evaluation of the model's performance.\"}\n",
      "{'question': \"Can I find information about the code's approach to handling long-running tasks and background jobs?\", 'answer': 'Yes, the code includes methods for submitting jobs, checking job status, and retrieving job results. It also includes a method for canceling jobs. Additionally, there is a method for sampling multiple outputs from a model, which could be useful for long-running tasks.'}\n",
      "{'question': 'How does Lamini AI handle requests for generating text that requires reasoning or decision-making based on given information?', 'answer': 'Lamini AI offers features for generating text that requires logical reasoning or inference beyond simple text generation. It can handle user prompts that involve complex reasoning or logical inference, and can generate text that captures the nuances of different cultural or regional variations.'}\n",
      "{'question': 'Does the `submit_job()` function expose any advanced training options such as learning rate schedules or early stopping?', 'answer': 'It is unclear which `submit_job()` function is being referred to as there is no such function defined in Lamini’s python library snippets. Please provide more information or context to answer the question accurately.'}\n",
      "{'question': 'Does the `add_data()` function support different data augmentation techniques or preprocessing options for training data?', 'answer': \"No, the `add_data()` function does not support different data augmentation techniques or preprocessing options for training data. It simply adds the provided examples to the program's list of examples.\"}\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "print(\"Pretrained dataset:\")\n",
    "top_n = itertools.islice(instruction_dataset_df,n)\n",
    "for i in top_n:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "467f4803-2848-4ab8-8653-d836cad33994",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"How can I evaluate the performance and quality of the generated text from Lamini models?There are several metrics that can be used to evaluate the performance and quality of generated text from Lamini models, including perplexity, BLEU score, and human evaluation. Perplexity measures how well the model predicts the next word in a sequence, while BLEU score measures the similarity between the generated text and a reference text. Human evaluation involves having human judges rate the quality of the generated text based on factors such as coherence, fluency, and relevance. It is recommended to use a combination of these metrics for a comprehensive evaluation of the model's performance.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = instruction_dataset_df.to_dict()\n",
    "text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "547c697e-6ce1-4dc6-9349-bbc9610282d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_qa = \"\"\"### Question:\n",
    "{question}\n",
    "### Answer: \n",
    "{answer}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c7761d2-9697-4da5-a343-f953238b663f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question:\n",
      "{question}\n",
      "### \n",
      "Answer: {answer}\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "56113118-dde9-49ff-bc33-814a0b65b0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question:\n",
      "How can I evaluate the performance and quality of the generated text from Lamini models?\n",
      "### Answer: \n",
      "There are several metrics that can be used to evaluate the performance and quality of generated text from Lamini models, including perplexity, BLEU score, and human evaluation. Perplexity measures how well the model predicts the next word in a sequence, while BLEU score measures the similarity between the generated text and a reference text. Human evaluation involves having human judges rate the quality of the generated text based on factors such as coherence, fluency, and relevance. It is recommended to use a combination of these metrics for a comprehensive evaluation of the model's performance.\n"
     ]
    }
   ],
   "source": [
    "question = examples[\"question\"][0]\n",
    "answer = examples[\"answer\"][0]\n",
    "\n",
    "text_with_prompt_template =  prompt_template_qa.format(question=question, answer = answer)\n",
    "print(text_with_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6fd21fc0-1a38-44bd-816f-6d20ff5b5223",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_q = \"\"\"### Question:\n",
    "{question}\n",
    "### Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7694c8ae-8ea4-499a-92cf-31d20c740fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = len(examples[\"question\"])\n",
    "finetuning_dataset_text_only = []\n",
    "finetuning_dataset_question_answer = []\n",
    "for i in range(num_examples):\n",
    "    question = examples[\"question\"][i]\n",
    "    answer = examples[\"answer\"][i]\n",
    "\n",
    "    text_with_prompt_template_qa = prompt_template_qa.format(question=question, answer = answer)\n",
    "    finetuning_dataset_text_only.append({\"text\": text_with_prompt_template_qa})\n",
    "\n",
    "    text_with_prompt_template_q = prompt_template_q.format(question=question)\n",
    "    finetuning_dataset_question_answer.append({\"question\": text_with_prompt_template_q ,\"answer\": answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1c63808b-d6c2-4a43-8663-d6419e5e8df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '### Question:\\n'\n",
      "         'How can I evaluate the performance and quality of the generated text '\n",
      "         'from Lamini models?\\n'\n",
      "         '### Answer: \\n'\n",
      "         'There are several metrics that can be used to evaluate the '\n",
      "         'performance and quality of generated text from Lamini models, '\n",
      "         'including perplexity, BLEU score, and human evaluation. Perplexity '\n",
      "         'measures how well the model predicts the next word in a sequence, '\n",
      "         'while BLEU score measures the similarity between the generated text '\n",
      "         'and a reference text. Human evaluation involves having human judges '\n",
      "         'rate the quality of the generated text based on factors such as '\n",
      "         'coherence, fluency, and relevance. It is recommended to use a '\n",
      "         'combination of these metrics for a comprehensive evaluation of the '\n",
      "         \"model's performance.\"}\n"
     ]
    }
   ],
   "source": [
    "pprint(finetuning_dataset_text_only[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a7d26c20-d331-4533-960d-db2569c96f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'There are several metrics that can be used to evaluate the '\n",
      "           'performance and quality of generated text from Lamini models, '\n",
      "           'including perplexity, BLEU score, and human evaluation. Perplexity '\n",
      "           'measures how well the model predicts the next word in a sequence, '\n",
      "           'while BLEU score measures the similarity between the generated '\n",
      "           'text and a reference text. Human evaluation involves having human '\n",
      "           'judges rate the quality of the generated text based on factors '\n",
      "           'such as coherence, fluency, and relevance. It is recommended to '\n",
      "           'use a combination of these metrics for a comprehensive evaluation '\n",
      "           \"of the model's performance.\",\n",
      " 'question': '### Question:\\n'\n",
      "             'How can I evaluate the performance and quality of the generated '\n",
      "             'text from Lamini models?\\n'\n",
      "             '### Answer:'}\n"
     ]
    }
   ],
   "source": [
    "pprint(finetuning_dataset_question_answer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6816620f-282e-433e-be1c-249268cf6b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(f'lamini_doc_processed.jsonl', 'w') as writer:\n",
    "    writer.write_all(finetuning_dataset_question_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3124ba75-7a48-4001-912a-8a9106539c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|████████████████████████████████████████████| 1260/1260 [00:00<00:00, 20674.21 examples/s]\n",
      "Generating test split: 100%|████████████████████████████████████████████████| 140/140 [00:00<00:00, 8665.28 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 140\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "finetuning_dataset_name = \"lamini/lamini_docs\"\n",
    "finetuning_dataset = load_dataset(finetuning_dataset_name)\n",
    "print(finetuning_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f431e64-1764-4f19-b632-f4d9b980d9ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bo_kernel",
   "language": "python",
   "name": "bo_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
