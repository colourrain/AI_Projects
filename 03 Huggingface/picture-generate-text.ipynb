{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25618028-c5c7-4a4d-9db7-324e7d5c66fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e05fe6869dd4ccc880b33106a5bce40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2cec276daf54dcc87d15fc92387ca66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BlipForConditionalGeneration\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5b80726-cf8e-49bb-badb-290082d1c5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5468207bbcd43ba95928f02fa235d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc140494e5634cfa9d46e4348bb7f250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3cbb47b77749288a1625d9658fa659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c1d44ce87534a57ab28996073e1fd0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22bae11314f2452b9316e440041a1c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c496614eb554674a2914532a2cd8438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "process = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22650432-cb74-4e4d-9485-6b385108cf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "img_url = \"https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg\"\n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "792a1f35-47d0-4461-92cf-c9954f4c2a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"a photograph of\"\n",
    "inputs = process(raw_image,text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99245396-f1e1-497e-b145-635a38ee8a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.generate(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "426eba8e-be2c-4a92-8663-4eef29b25e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30522,  1037,  9982,  1997,  1037,  2450,  1998,  2014,  3899,  2006,\n",
       "         1996,  3509,   102])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2fb3adb-3235-47b0-b418-0d2ddb4e0e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs2 = process.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e1a4062-e2cb-4f97-ab20-0b7a404d9bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a photograph of a woman and her dog on the beach'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
