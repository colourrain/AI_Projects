{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2fa0033-e8b5-4e19-8885-594ea6663ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "\n",
    "from pprint import pprint\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9104a82-5b21-4b90-b100-f2992f04b585",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/pythia-70m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ed8133d-f8a8-47ef-ab2f-4686b47151e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12764, 13, 849, 403, 368, 32]\n"
     ]
    }
   ],
   "source": [
    "text = \"Hi, how are you?\"\n",
    "encode_text = tokenizer(text)[\"input_ids\"]\n",
    "pprint(encode_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01e8c20a-63a7-45b0-9a0c-618afe448dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, how are you?\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(encode_text)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b1d8d8e-35e8-40f9-a9bc-6146abc6f192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12764, 13, 849, 403, 368, 32], [42, 717, 1175], [4374]]\n"
     ]
    }
   ],
   "source": [
    "list_texts = [\"Hi, how are you?\", \"I am good\", \"Yes\"]\n",
    "encoded_texts = tokenizer(list_texts)\n",
    "print(encoded_texts[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aad9aa33-4bbc-460e-951d-e5b5d1c8114e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using padding: [[12764, 13, 849, 403, 368, 32], [42, 717, 1175, 0, 0, 0], [4374, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "encoded_texts_longest = tokenizer(list_texts, padding=True)\n",
    "print(\"Using padding:\", encoded_texts_longest[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3d02586-fd63-44b1-b297-32a7a34e0a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using truncation: [[12764, 13, 849], [42, 717, 1175], [4374]]\n"
     ]
    }
   ],
   "source": [
    "encoded_texts_truncation = tokenizer(list_texts, max_length=3, truncation=True)\n",
    "print(\"Using truncation:\", encoded_texts_truncation[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4490caa2-553c-41f6-b45c-6c9f61b6dc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using truncation: [[403, 368, 32], [42, 717, 1175], [4374]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.truncation_side = \"left\"\n",
    "encoded_texts_truncation_left = tokenizer(list_texts, max_length=3, truncation=True)\n",
    "print(\"Using truncation:\", encoded_texts_truncation_left[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e2890f0-16da-4f55-b3cd-c8547ab7003c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using truncation: [[403, 368, 32], [42, 717, 1175], [4374, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "encoded_texts_truncation_both = tokenizer(list_texts, max_length=3, truncation=True, padding=True)\n",
    "print(\"Using truncation:\", encoded_texts_truncation_both[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0634c294-965c-43b2-91b2-8b71662426ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer'],\n",
      "    num_rows: 1400\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "instruction_dataset_df = load_dataset(\"kotzeje/lamini_docs.jsonl\", split = \"train\", trust_remote_code=True)\n",
    "print(instruction_dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fee99e8-94a4-4823-a42d-b341129d7f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'There are several metrics that can be used to evaluate the '\n",
      "           'performance and quality of generated text from Lamini models, '\n",
      "           'including perplexity, BLEU score, and human evaluation. Perplexity '\n",
      "           'measures how well the model predicts the next word in a sequence, '\n",
      "           'while BLEU score measures the similarity between the generated '\n",
      "           'text and a reference text. Human evaluation involves having human '\n",
      "           'judges rate the quality of the generated text based on factors '\n",
      "           'such as coherence, fluency, and relevance. It is recommended to '\n",
      "           'use a combination of these metrics for a comprehensive evaluation '\n",
      "           \"of the model's performance.\",\n",
      " 'question': '### Question:\\n'\n",
      "             'How can I evaluate the performance and quality of the generated '\n",
      "             'text from Lamini models?\\n'\n",
      "             '### Answer:'}\n"
     ]
    }
   ],
   "source": [
    "examples = instruction_dataset_df.to_dict()\n",
    "prompt_template = \"\"\"### Question:\n",
    "{question}\n",
    "### Answer:\"\"\"\n",
    "\n",
    "num_examples = len(examples[\"question\"])\n",
    "finetuning_dataset = []\n",
    "for i in range(num_examples):\n",
    "    question = examples[\"question\"][i]\n",
    "    answer = examples[\"answer\"][i]\n",
    "\n",
    "    text_with_prompt_template = prompt_template.format(question=question)\n",
    "    finetuning_dataset.append({\"question\": text_with_prompt_template ,\"answer\": answer})\n",
    "print(\"One datapoint in the finetuning dataset:\")\n",
    "pprint(finetuning_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c02c2bf-1314-419d-a09c-53326cd2a40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4118 19782    27   187  2347   476   309  7472   253  3045   285  3290\n",
      "    273   253  4561  2505   432   418  4988    74  3210    32   187  4118\n",
      "  37741    27  2512   403  2067 17082   326   476   320   908   281  7472\n",
      "    253  3045   285  3290   273  4561  2505   432   418  4988    74  3210\n",
      "     13  1690 44229   414    13   378  1843    54  4868    13   285  1966\n",
      "   7103    15  3545 12813   414  5593   849   973   253  1566 26295   253\n",
      "   1735  3159   275   247  3425    13  1223   378  1843    54  4868  5593\n",
      "    253 14259   875   253  4561  2505   285   247  3806  2505    15  8801\n",
      "   7103  8687  1907  1966 16006  2281   253  3290   273   253  4561  2505\n",
      "   1754   327  2616   824   347 25253    13  2938  1371    13   285 17200\n",
      "     15   733   310  8521   281   897   247  5019   273   841 17082   323\n",
      "    247 11088  7103   273   253  1566   434  3045    15]]\n"
     ]
    }
   ],
   "source": [
    "text = finetuning_dataset[0][\"question\"] + finetuning_dataset[0][\"answer\"]\n",
    "tokenizer_inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"np\",\n",
    "    padding=True\n",
    ")\n",
    "print(tokenizer_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1f8e23f-7987-4807-8be4-4ac4ed574f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 2048\n",
    "max_length = min(\n",
    "    tokenizer_inputs[\"input_ids\"].shape[1],\n",
    "    max_length\n",
    ")\n",
    "tokenizer_inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"np\",\n",
    "    max_length=max_length,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b36aed7b-d1f7-4d92-96fb-8aa012b76e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_function(examples):\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        padding=True\n",
    "    )\n",
    "   \n",
    "    max_length = min(\n",
    "        tokenizer_inputs[\"input_ids\"].shape[1],\n",
    "        2048\n",
    "    )\n",
    "    tokenizer_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        max_length=max_length,\n",
    "        truncation=True\n",
    "    )\n",
    "    return tokenizer_inputs\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b3a5b8a-fdcc-437b-ad25-754aab7088fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'There are several metrics that can be used to evaluate the '\n",
      "           'performance and quality of generated text from Lamini models, '\n",
      "           'including perplexity, BLEU score, and human evaluation. Perplexity '\n",
      "           'measures how well the model predicts the next word in a sequence, '\n",
      "           'while BLEU score measures the similarity between the generated '\n",
      "           'text and a reference text. Human evaluation involves having human '\n",
      "           'judges rate the quality of the generated text based on factors '\n",
      "           'such as coherence, fluency, and relevance. It is recommended to '\n",
      "           'use a combination of these metrics for a comprehensive evaluation '\n",
      "           \"of the model's performance.\",\n",
      " 'question': 'How can I evaluate the performance and quality of the generated '\n",
      "             'text from Lamini models?'}\n"
     ]
    }
   ],
   "source": [
    "pprint(instruction_dataset_df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e1f4a14-c0b8-466f-880a-1e7c6402f79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a9344b13684278b8cc674b6fb6d9ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1400\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenized_dataset = instruction_dataset_df.map(\n",
    "    tokenizer_function,\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    drop_last_batch=True\n",
    ")\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a62c768-665c-49b7-9f6b-8eddf65e37e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'There are several metrics that can be used to evaluate the '\n",
      "           'performance and quality of generated text from Lamini models, '\n",
      "           'including perplexity, BLEU score, and human evaluation. Perplexity '\n",
      "           'measures how well the model predicts the next word in a sequence, '\n",
      "           'while BLEU score measures the similarity between the generated '\n",
      "           'text and a reference text. Human evaluation involves having human '\n",
      "           'judges rate the quality of the generated text based on factors '\n",
      "           'such as coherence, fluency, and relevance. It is recommended to '\n",
      "           'use a combination of these metrics for a comprehensive evaluation '\n",
      "           \"of the model's performance.\",\n",
      " 'attention_mask': [1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1],\n",
      " 'input_ids': [4118,\n",
      "               19782,\n",
      "               27,\n",
      "               187,\n",
      "               2347,\n",
      "               476,\n",
      "               309,\n",
      "               7472,\n",
      "               253,\n",
      "               3045,\n",
      "               285,\n",
      "               3290,\n",
      "               273,\n",
      "               253,\n",
      "               4561,\n",
      "               2505,\n",
      "               432,\n",
      "               418,\n",
      "               4988,\n",
      "               74,\n",
      "               3210,\n",
      "               32,\n",
      "               187,\n",
      "               4118,\n",
      "               37741,\n",
      "               27,\n",
      "               2512,\n",
      "               403,\n",
      "               2067,\n",
      "               17082,\n",
      "               326,\n",
      "               476,\n",
      "               320,\n",
      "               908,\n",
      "               281,\n",
      "               7472,\n",
      "               253,\n",
      "               3045,\n",
      "               285,\n",
      "               3290,\n",
      "               273,\n",
      "               4561,\n",
      "               2505,\n",
      "               432,\n",
      "               418,\n",
      "               4988,\n",
      "               74,\n",
      "               3210,\n",
      "               13,\n",
      "               1690,\n",
      "               44229,\n",
      "               414,\n",
      "               13,\n",
      "               378,\n",
      "               1843,\n",
      "               54,\n",
      "               4868,\n",
      "               13,\n",
      "               285,\n",
      "               1966,\n",
      "               7103,\n",
      "               15,\n",
      "               3545,\n",
      "               12813,\n",
      "               414,\n",
      "               5593,\n",
      "               849,\n",
      "               973,\n",
      "               253,\n",
      "               1566,\n",
      "               26295,\n",
      "               253,\n",
      "               1735,\n",
      "               3159,\n",
      "               275,\n",
      "               247,\n",
      "               3425,\n",
      "               13,\n",
      "               1223,\n",
      "               378,\n",
      "               1843,\n",
      "               54,\n",
      "               4868,\n",
      "               5593,\n",
      "               253,\n",
      "               14259,\n",
      "               875,\n",
      "               253,\n",
      "               4561,\n",
      "               2505,\n",
      "               285,\n",
      "               247,\n",
      "               3806,\n",
      "               2505,\n",
      "               15,\n",
      "               8801,\n",
      "               7103,\n",
      "               8687,\n",
      "               1907,\n",
      "               1966,\n",
      "               16006,\n",
      "               2281,\n",
      "               253,\n",
      "               3290,\n",
      "               273,\n",
      "               253,\n",
      "               4561,\n",
      "               2505,\n",
      "               1754,\n",
      "               327,\n",
      "               2616,\n",
      "               824,\n",
      "               347,\n",
      "               25253,\n",
      "               13,\n",
      "               2938,\n",
      "               1371,\n",
      "               13,\n",
      "               285,\n",
      "               17200,\n",
      "               15,\n",
      "               733,\n",
      "               310,\n",
      "               8521,\n",
      "               281,\n",
      "               897,\n",
      "               247,\n",
      "               5019,\n",
      "               273,\n",
      "               841,\n",
      "               17082,\n",
      "               323,\n",
      "               247,\n",
      "               11088,\n",
      "               7103,\n",
      "               273,\n",
      "               253,\n",
      "               1566,\n",
      "               434,\n",
      "               3045,\n",
      "               15],\n",
      " 'question': 'How can I evaluate the performance and quality of the generated '\n",
      "             'text from Lamini models?'}\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenized_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebbb820-9247-48ff-9585-e181f286af98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bo_kernel",
   "language": "python",
   "name": "bo_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
