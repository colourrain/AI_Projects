{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac0e7a0e-d74b-4d1b-b3fa-a292feaadba6",
   "metadata": {},
   "source": [
    "# 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96727cd0-3da5-48e1-a7db-4ac302e8b125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10adead9-e8b0-464d-86c9-326efc40eaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /datasets/opencsg/chinese-fineweb-edu/resolve/main/README.md (Caused by ConnectTimeoutError(<HTTPSConnection(host='huggingface.co', port=443) at 0x15cfe3e00>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 38516ad6-c581-4b8d-8954-7117329d0ce3)')' thrown while requesting HEAD https://huggingface.co/datasets/opencsg/chinese-fineweb-edu/resolve/main/README.md\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /datasets/opencsg/chinese-fineweb-edu/resolve/main/README.md (Caused by ConnectTimeoutError(<HTTPSConnection(host='huggingface.co', port=443) at 0x15d052e90>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: b6813a76-0ed2-442d-b0c8-78c67ec616b1)')' thrown while requesting HEAD https://huggingface.co/datasets/opencsg/chinese-fineweb-edu/resolve/main/README.md\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /datasets/opencsg/chinese-fineweb-edu/resolve/main/README.md (Caused by ConnectTimeoutError(<HTTPSConnection(host='huggingface.co', port=443) at 0x15d053390>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 26a3655d-8d31-4a17-ab34-3c1c40934dde)')' thrown while requesting HEAD https://huggingface.co/datasets/opencsg/chinese-fineweb-edu/resolve/main/README.md\n",
      "Retrying in 4s [Retry 3/5].\n"
     ]
    }
   ],
   "source": [
    "pretrained_datasets = load_dataset(\"opencsg/chinese-fineweb-edu\", split = \"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad525826-c22c-462b-9bae-f72fd2f9501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "print(\"Pretrained dataset:\")\n",
    "top_n = itertools.islice(pretrained_datasets,n)\n",
    "for i in top_n:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c4805eb-d27d-4655-872d-3d42a4251be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer'],\n",
      "    num_rows: 1400\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "instruction_dataset_df = load_dataset(\"kotzeje/lamini_docs.jsonl\", split = \"train\")\n",
    "print(instruction_dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5ca05d0-0001-4e39-bfd5-471a3ab1ea4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained dataset:\n",
      "{'question': 'How can I evaluate the performance and quality of the generated text from Lamini models?', 'answer': \"There are several metrics that can be used to evaluate the performance and quality of generated text from Lamini models, including perplexity, BLEU score, and human evaluation. Perplexity measures how well the model predicts the next word in a sequence, while BLEU score measures the similarity between the generated text and a reference text. Human evaluation involves having human judges rate the quality of the generated text based on factors such as coherence, fluency, and relevance. It is recommended to use a combination of these metrics for a comprehensive evaluation of the model's performance.\"}\n",
      "{'question': \"Can I find information about the code's approach to handling long-running tasks and background jobs?\", 'answer': 'Yes, the code includes methods for submitting jobs, checking job status, and retrieving job results. It also includes a method for canceling jobs. Additionally, there is a method for sampling multiple outputs from a model, which could be useful for long-running tasks.'}\n",
      "{'question': 'How does Lamini AI handle requests for generating text that requires reasoning or decision-making based on given information?', 'answer': 'Lamini AI offers features for generating text that requires logical reasoning or inference beyond simple text generation. It can handle user prompts that involve complex reasoning or logical inference, and can generate text that captures the nuances of different cultural or regional variations.'}\n",
      "{'question': 'Does the `submit_job()` function expose any advanced training options such as learning rate schedules or early stopping?', 'answer': 'It is unclear which `submit_job()` function is being referred to as there is no such function defined in Lamini’s python library snippets. Please provide more information or context to answer the question accurately.'}\n",
      "{'question': 'Does the `add_data()` function support different data augmentation techniques or preprocessing options for training data?', 'answer': \"No, the `add_data()` function does not support different data augmentation techniques or preprocessing options for training data. It simply adds the provided examples to the program's list of examples.\"}\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "print(\"Pretrained dataset:\")\n",
    "top_n = itertools.islice(instruction_dataset_df,n)\n",
    "for i in top_n:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "467f4803-2848-4ab8-8653-d836cad33994",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"How can I evaluate the performance and quality of the generated text from Lamini models?There are several metrics that can be used to evaluate the performance and quality of generated text from Lamini models, including perplexity, BLEU score, and human evaluation. Perplexity measures how well the model predicts the next word in a sequence, while BLEU score measures the similarity between the generated text and a reference text. Human evaluation involves having human judges rate the quality of the generated text based on factors such as coherence, fluency, and relevance. It is recommended to use a combination of these metrics for a comprehensive evaluation of the model's performance.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = instruction_dataset_df.to_dict()\n",
    "text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "547c697e-6ce1-4dc6-9349-bbc9610282d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_qa = \"\"\"### Question:\n",
    "{question}\n",
    "### Answer: \n",
    "{answer}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c7761d2-9697-4da5-a343-f953238b663f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question:\n",
      "{question}\n",
      "### Answer: \n",
      "{answer}\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56113118-dde9-49ff-bc33-814a0b65b0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question:\n",
      "How can I evaluate the performance and quality of the generated text from Lamini models?\n",
      "### Answer: \n",
      "There are several metrics that can be used to evaluate the performance and quality of generated text from Lamini models, including perplexity, BLEU score, and human evaluation. Perplexity measures how well the model predicts the next word in a sequence, while BLEU score measures the similarity between the generated text and a reference text. Human evaluation involves having human judges rate the quality of the generated text based on factors such as coherence, fluency, and relevance. It is recommended to use a combination of these metrics for a comprehensive evaluation of the model's performance.\n"
     ]
    }
   ],
   "source": [
    "question = examples[\"question\"][0]\n",
    "answer = examples[\"answer\"][0]\n",
    "\n",
    "text_with_prompt_template =  prompt_template_qa.format(question=question, answer = answer)\n",
    "print(text_with_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fd21fc0-1a38-44bd-816f-6d20ff5b5223",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_q = \"\"\"### Question:\n",
    "{question}\n",
    "### Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7694c8ae-8ea4-499a-92cf-31d20c740fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = len(examples[\"question\"])\n",
    "finetuning_dataset_text_only = []\n",
    "finetuning_dataset_question_answer = []\n",
    "for i in range(num_examples):\n",
    "    question = examples[\"question\"][i]\n",
    "    answer = examples[\"answer\"][i]\n",
    "\n",
    "    text_with_prompt_template_qa = prompt_template_qa.format(question=question, answer = answer)\n",
    "    finetuning_dataset_text_only.append({\"text\": text_with_prompt_template_qa})\n",
    "\n",
    "    text_with_prompt_template_q = prompt_template_q.format(question=question)\n",
    "    finetuning_dataset_question_answer.append({\"question\": text_with_prompt_template_q ,\"answer\": answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c63808b-d6c2-4a43-8663-d6419e5e8df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '### Question:\\n'\n",
      "         'How can I evaluate the performance and quality of the generated text '\n",
      "         'from Lamini models?\\n'\n",
      "         '### Answer: \\n'\n",
      "         'There are several metrics that can be used to evaluate the '\n",
      "         'performance and quality of generated text from Lamini models, '\n",
      "         'including perplexity, BLEU score, and human evaluation. Perplexity '\n",
      "         'measures how well the model predicts the next word in a sequence, '\n",
      "         'while BLEU score measures the similarity between the generated text '\n",
      "         'and a reference text. Human evaluation involves having human judges '\n",
      "         'rate the quality of the generated text based on factors such as '\n",
      "         'coherence, fluency, and relevance. It is recommended to use a '\n",
      "         'combination of these metrics for a comprehensive evaluation of the '\n",
      "         \"model's performance.\"}\n"
     ]
    }
   ],
   "source": [
    "pprint(finetuning_dataset_text_only[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7d26c20-d331-4533-960d-db2569c96f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'There are several metrics that can be used to evaluate the '\n",
      "           'performance and quality of generated text from Lamini models, '\n",
      "           'including perplexity, BLEU score, and human evaluation. Perplexity '\n",
      "           'measures how well the model predicts the next word in a sequence, '\n",
      "           'while BLEU score measures the similarity between the generated '\n",
      "           'text and a reference text. Human evaluation involves having human '\n",
      "           'judges rate the quality of the generated text based on factors '\n",
      "           'such as coherence, fluency, and relevance. It is recommended to '\n",
      "           'use a combination of these metrics for a comprehensive evaluation '\n",
      "           \"of the model's performance.\",\n",
      " 'question': '### Question:\\n'\n",
      "             'How can I evaluate the performance and quality of the generated '\n",
      "             'text from Lamini models?\\n'\n",
      "             '### Answer:'}\n"
     ]
    }
   ],
   "source": [
    "pprint(finetuning_dataset_question_answer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6816620f-282e-433e-be1c-249268cf6b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(f'lamini_doc_processed.jsonl', 'w') as writer:\n",
    "    writer.write_all(finetuning_dataset_question_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3124ba75-7a48-4001-912a-8a9106539c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57587207c64d466fa8a9f08d5506c4b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/577 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e695d8cc66c4a2e9440f21e2e01b8ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001-5cdebbc48da413(…):   0%|          | 0.00/615k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e638f37d20e4bd881db3485bba78797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001-4c77a066a883f33(…):   0%|          | 0.00/83.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d33b088ea0243b68e844137b6bc5085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1260 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0887f781d5904e83af6f7d40d499eadd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 140\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "finetuning_dataset_name = \"lamini/lamini_docs\"\n",
    "finetuning_dataset = load_dataset(finetuning_dataset_name)\n",
    "print(finetuning_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f431e64-1764-4f19-b632-f4d9b980d9ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tranformer)",
   "language": "python",
   "name": "transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
