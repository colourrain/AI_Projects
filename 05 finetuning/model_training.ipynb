{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "82789d9b-d0d9-42ae-9c69-00de6ea5e69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import tempfile\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from transformers import Trainer\n",
    "from datasets import load_dataset\n",
    "#from utilities import *\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "global_config = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "53e992bb-13ff-4a7f-8024-8faafb31d1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/lamini_docs_processed.jsonl\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"lamini_docs_processed.jsonl\"\n",
    "dataset_path = f\"/{dataset_name}\"\n",
    "use_hf = False\n",
    "print(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "eeccb692-5d1e-46ca-ae02-4045ff0617a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer'],\n",
      "    num_rows: 1400\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset(\"kotzeje/lamini_docs.jsonl\", split = \"train\", trust_remote_code=True)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e702ec8a-9c29-4ed2-a64d-2a5667aac50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/pythia-70m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5363916e-9111-4359-a2a4-e79f95c0381e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n"
     ]
    }
   ],
   "source": [
    "training_config = {\n",
    "    \"model\":{\n",
    "        \"pretrained_name\": model_name,\n",
    "        \"max_length\" : 2048\n",
    "    },\n",
    "    \"datasets\": {\n",
    "        \"use_hf\" : use_hf,\n",
    "        \"path\" : dataset_path\n",
    "    },\n",
    "    \"verbose\": True\n",
    "}\n",
    "print(training_config[\"model\"][\"max_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "770f1911-c845-4f29-9626-a7d736ce4898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and split data\n",
    "def load_dataset1(dataset_path, tokenizer):\n",
    "    random.seed(42)\n",
    "    print(dataset_path)\n",
    "    finetuning_dataset_loaded = datasets.load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    max_length = training_config[\"model\"][\"max_length\"]\n",
    "    tokenized_dataset = finetuning_dataset_loaded.map(\n",
    "        get_tokenize_function(tokenizer, max_length), # returns tokenize_function\n",
    "        batched=True,\n",
    "        batch_size=1,\n",
    "        drop_last_batch=True\n",
    "    )\n",
    "    tokenized_dataset = tokenized_dataset.with_format(\"torch\")\n",
    "    split_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=123)\n",
    "    return split_dataset\n",
    "# Wrapper for data load, split, tokenize for training\n",
    "def tokenize_and_split_data(training_config, tokenizer):\n",
    "    dataset_path = training_config[\"datasets\"][\"path\"]\n",
    "    use_hf = training_config[\"datasets\"][\"use_hf\"]\n",
    "    print(\"tokenize\", use_hf, dataset_path)\n",
    "    if use_hf:\n",
    "        dataset = datasets.load_dataset(dataset_path)\n",
    "    else:\n",
    "        dataset = load_dataset1(dataset_path, tokenizer)\n",
    "        train_dataset = dataset[\"train\"]\n",
    "        test_dataset = dataset[\"test\"]\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b7d40339-edde-42ff-a309-89ea5262786e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize False /lamini_docs_processed.jsonl\n",
      "/lamini_docs_processed.jsonl\n",
      "{'answer': \"To leverage Lamini's features for improving model performance or \"\n",
      "           'generalization, you can use the pre-trained models and embeddings '\n",
      "           'provided by Lamini, or fine-tune them on your specific task. '\n",
      "           \"Finally, you can use Lamini's model selection and hyperparameter \"\n",
      "           'tuning tools to find the best model architecture and '\n",
      "           'hyperparameters for your task.',\n",
      " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1]),\n",
      " 'input_ids': tensor([ 4118, 19782,    27,   187,  2347,   476,   309, 25057,   418,  4988,\n",
      "           74,   434,  3386,   281,  3157,   253,  3045,   390, 26647,   273,\n",
      "          247, 32176,  1566,    32,   187,  4118, 37741,    27,  1992, 25057,\n",
      "          418,  4988,    74,   434,  3386,   323, 11138,  1566,  3045,   390,\n",
      "        26647,    13,   368,   476,   897,   253,   638,    14, 32927,  3210,\n",
      "          285, 46234,  2530,   407,   418,  4988,    74,    13,   390,  4030,\n",
      "           14,    85,  2517,   731,   327,   634,  2173,  4836,    15,  6610,\n",
      "           13,   368,   476,   897,   418,  4988,    74,   434,  1566,  5438,\n",
      "          285,  4373, 19484, 25184,  5657,   281,  1089,   253,  1682,  1566,\n",
      "        10336,   285,  4373, 22041,   323,   634,  4836,    15]),\n",
      " 'labels': tensor([ 4118, 19782,    27,   187,  2347,   476,   309, 25057,   418,  4988,\n",
      "           74,   434,  3386,   281,  3157,   253,  3045,   390, 26647,   273,\n",
      "          247, 32176,  1566,    32,   187,  4118, 37741,    27,  1992, 25057,\n",
      "          418,  4988,    74,   434,  3386,   323, 11138,  1566,  3045,   390,\n",
      "        26647,    13,   368,   476,   897,   253,   638,    14, 32927,  3210,\n",
      "          285, 46234,  2530,   407,   418,  4988,    74,    13,   390,  4030,\n",
      "           14,    85,  2517,   731,   327,   634,  2173,  4836,    15,  6610,\n",
      "           13,   368,   476,   897,   418,  4988,    74,   434,  1566,  5438,\n",
      "          285,  4373, 19484, 25184,  5657,   281,  1089,   253,  1682,  1566,\n",
      "        10336,   285,  4373, 22041,   323,   634,  4836,    15]),\n",
      " 'question': '### Question:\\n'\n",
      "             \"How can I leverage Lamini's features to improve the performance \"\n",
      "             'or generalization of a customized model?\\n'\n",
      "             '### Answer:'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "    \n",
    "tokenizer =  AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_dataset, test_dataset = tokenize_and_split_data(training_config,tokenizer)\n",
    "pprint(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ed0191f1-47a0-4f5c-bf14-57393d8a2e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d4066b32-dc19-42bc-868e-b95de661791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_conunt = torch.cuda.device_count()\n",
    "if device_conunt >0:\n",
    "    logger.debug(\"Select GPU device\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    logger.debug(\"Select CPU device\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4c76cd27-b2b4-4dce-a7e6-be361fbd626d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d57fc687-6f8e-41de-8a0c-4c65458c2bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_out_tokens=100):\n",
    "    # Tokenize\n",
    "    input_ids = tokenizer.encode(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_tokens\n",
    "    )\n",
    "\n",
    "    # generate\n",
    "    device = model.device\n",
    "    generated_tokens_with_prompt = model.generate(\n",
    "        input_ids=input_ids.to(device),\n",
    "        max_length=max_out_tokens\n",
    "    )\n",
    "\n",
    "    # decode\n",
    "    generated_tokens_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt)\n",
    "\n",
    "    # Strip the prompt\n",
    "    generated_text_answer = generated_tokens_with_prompt[0][len(text):]\n",
    "\n",
    "    return generated_text_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "579c84bd-4eff-4344-b506-c0490947b1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question input: ### Question:\n",
      "How can I leverage Lamini's features to improve the performance or generalization of a customized model?\n",
      "### Answer:\n",
      "Correct answer from Lamini doc: To leverage Lamini's features for improving model performance or generalization, you can use the pre-trained models and embeddings provided by Lamini, or fine-tune them on your specific task. Finally, you can use Lamini's model selection and hyperparameter tuning tools to find the best model architecture and hyperparameters for your task.\n",
      "\n",
      "\n",
      "A:\n",
      "\n",
      "I would like to use the Lamini feature to improve the performance of a custom model.\n",
      "\n",
      "A:\n",
      "\n",
      "I would like to use the Lamini feature to improve the performance of a custom model.\n",
      "\n",
      "A:\n",
      "\n",
      "I would like to use the Lamini feature to improve the performance of a\n"
     ]
    }
   ],
   "source": [
    "train_text = train_dataset[0][\"question\"]\n",
    "print(\"Question input:\", train_text)\n",
    "print(f\"Correct answer from Lamini doc: {train_dataset[0]['answer']}\" )\n",
    "print(inference(train_text, base_model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "216484fa-b03b-4be4-88c0-235315332ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 10\n",
    "trained_model_name = f\"lamini_doc_{max_steps}_steps\"\n",
    "output_dir = trained_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c4de5ea6-a844-4ec1-be8f-d3ba40473952",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate= 1.0e-5,\n",
    "    num_train_epochs = 10,\n",
    "    max_steps=max_steps,\n",
    "    per_device_train_batch_size = 1,\n",
    "    output_dir = output_dir,\n",
    "    remove_unused_columns=False,\n",
    "\n",
    "    overwrite_output_dir=False,\n",
    "    disable_tqdm=False, #Enable/disable progress bars\n",
    "    eval_steps=120, # number of update steps between two evaluations\n",
    "    save_steps =120, # After # steps model is saved\n",
    "    warmup_steps = 1, # no of warmup steps for learning rate scheduler\n",
    "    per_device_eval_batch_size=1, # batch size for evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    logging_strategy =\"steps\",\n",
    "    logging_steps=1,\n",
    "    optim=\"adafactor\",\n",
    "    gradient_accumulation_steps = 4,\n",
    "    gradient_checkpointing = False,\n",
    "\n",
    "    # Parameters for early stopping\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8f5b3586-7553-45dd-b298-cdafbbb35c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 512)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXAttention(\n",
      "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  )\n",
      "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
      ")\n",
      "Memory footprint 0.281706528 GB\n",
      "Flops 2195.667812352 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "model_flops = (\n",
    "    base_model.floating_point_ops(\n",
    "        {\n",
    "            \"input_ids\": torch.zeros(\n",
    "                (1, training_config[\"model\"][\"max_length\"])\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    * training_args.gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "print(base_model)\n",
    "print(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\")\n",
    "print(\"Flops\", model_flops / 1e9, \"GFLOPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "dd1af792-39c8-4688-8f9b-9150c0b306e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': \"To leverage Lamini's features for improving model performance or \"\n",
      "           'generalization, you can use the pre-trained models and embeddings '\n",
      "           'provided by Lamini, or fine-tune them on your specific task. '\n",
      "           \"Finally, you can use Lamini's model selection and hyperparameter \"\n",
      "           'tuning tools to find the best model architecture and '\n",
      "           'hyperparameters for your task.',\n",
      " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1]),\n",
      " 'input_ids': tensor([ 4118, 19782,    27,   187,  2347,   476,   309, 25057,   418,  4988,\n",
      "           74,   434,  3386,   281,  3157,   253,  3045,   390, 26647,   273,\n",
      "          247, 32176,  1566,    32,   187,  4118, 37741,    27,  1992, 25057,\n",
      "          418,  4988,    74,   434,  3386,   323, 11138,  1566,  3045,   390,\n",
      "        26647,    13,   368,   476,   897,   253,   638,    14, 32927,  3210,\n",
      "          285, 46234,  2530,   407,   418,  4988,    74,    13,   390,  4030,\n",
      "           14,    85,  2517,   731,   327,   634,  2173,  4836,    15,  6610,\n",
      "           13,   368,   476,   897,   418,  4988,    74,   434,  1566,  5438,\n",
      "          285,  4373, 19484, 25184,  5657,   281,  1089,   253,  1682,  1566,\n",
      "        10336,   285,  4373, 22041,   323,   634,  4836,    15]),\n",
      " 'labels': tensor([ 4118, 19782,    27,   187,  2347,   476,   309, 25057,   418,  4988,\n",
      "           74,   434,  3386,   281,  3157,   253,  3045,   390, 26647,   273,\n",
      "          247, 32176,  1566,    32,   187,  4118, 37741,    27,  1992, 25057,\n",
      "          418,  4988,    74,   434,  3386,   323, 11138,  1566,  3045,   390,\n",
      "        26647,    13,   368,   476,   897,   253,   638,    14, 32927,  3210,\n",
      "          285, 46234,  2530,   407,   418,  4988,    74,    13,   390,  4030,\n",
      "           14,    85,  2517,   731,   327,   634,  2173,  4836,    15,  6610,\n",
      "           13,   368,   476,   897,   418,  4988,    74,   434,  1566,  5438,\n",
      "          285,  4373, 19484, 25184,  5657,   281,  1089,   253,  1682,  1566,\n",
      "        10336,   285,  4373, 22041,   323,   634,  4836,    15]),\n",
      " 'question': '### Question:\\n'\n",
      "             \"How can I leverage Lamini's features to improve the performance \"\n",
      "             'or generalization of a customized model?\\n'\n",
      "             '### Answer:'}\n",
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 140\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "pprint(train_dataset[0])\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "048496ce-3746-4e60-bcfa-8af4a6123896",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = base_model,\n",
    "    #model_flops = model_flops,\n",
    "    #total_steps = max_steps,\n",
    "    args = training_args,\n",
    "    train_dataset = test_dataset,\n",
    "    eval_dataset = test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b7e7cfb5-b53d-4739-a5d8-eae52dccf771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:21, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_output = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "85b9978e-5cc4-4821-94ba-a0628c5ea0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model to: lamini_doc_10_steps/final\n"
     ]
    }
   ],
   "source": [
    "save_dir = f'{output_dir}/final'\n",
    "\n",
    "trainer.save_model(save_dir)\n",
    "print(\"saved model to:\", save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "97b13df6-16a5-4672-844c-8c930c8967ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5f414393-7642-4532-a4d3-f4b272aacc8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_slightly_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e07bf41b-527a-46a3-b8fc-3f5826c2683f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question input(test): ### Question:\n",
      "Is it possible to fine-tune Lamini on a specific dataset for text generation in legal documents?\n",
      "### Answer:\n",
      "finetuned slightly model 's answer:'\n",
      "('Yes, it is possible to fine-tune Lamini on a specific dataset for text '\n",
      " 'generation in legal documents.\\n'\n",
      " '### Answer:Yes, it is possible to fine-tune Lamini on a specific dataset for '\n",
      " 'text generation in legal documents.\\n'\n",
      " '### Answer:Yes, it is possible to fine-tune Lamini on')\n"
     ]
    }
   ],
   "source": [
    "test_question = test_dataset[0][\"question\"]\n",
    "print(\"question input(test):\", test_question)\n",
    "print(\"finetuned slightly model 's answer:'\")\n",
    "pprint(inference(test_question, finetuned_slightly_model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b5009595-ab31-4242-a5b3-646e20e039c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target answer output (test): Laminiâ€™s LLM Engine can help you fine-tune any model on huggingface or any OpenAI model.\n"
     ]
    }
   ],
   "source": [
    "test_answer = test_dataset[0]['answer']\n",
    "print(\"Target answer output (test):\", test_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b64c624-98aa-4b41-bde0-645fd9bd71b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bo_kernel",
   "language": "python",
   "name": "bo_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
