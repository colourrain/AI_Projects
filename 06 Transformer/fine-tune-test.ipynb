{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661da5df-2d80-4f59-b60a-6d907937af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"fill-mask\", model=\"google-bert/bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12634df3-f0b3-4cd4-ba7b-ceafa019ee71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-chinese were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-chinese\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d54db3db-d1cb-4d58-9188-1e729b38a715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = 'bert-base-chinese'\n",
    "save_directory = './bert-base-chinese'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "# model.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebbaf118-52ff-4eea-9953-433f9928d41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert-base-chinese were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ./bert-base-chinese and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "D:\\anaconda3\\envs\\mykerasenv\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1833: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 768])\n",
      "{'input_ids': tensor([[ 101,  782, 4495, 6421, 1963,  862, 2458, 1993,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, AutoModelForSequenceClassification, AutoModelForQuestionAnswering, AutoModelWithLMHead\n",
    "import torch\n",
    "model_path = './bert-base-chinese'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "model_LM = AutoModelForCausalLM.from_pretrained(model_path, is_decoder=True)\n",
    "model_classfication = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "model_QA = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
    "model_withhead = AutoModelWithLMHead.from_pretrained(model_path)\n",
    "\n",
    "input_text = \"人生该如何开始\"\n",
    "inputs = tokenizer(input_text, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "print(last_hidden_states.shape)\n",
    "print(inputs)\n",
    "output_withhead = model_withhead(** inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66b0692-8bae-46d9-8749-e9666cb16eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e8779e9-494e-4f94-b0fc-85db31b7fc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 21128])\n"
     ]
    }
   ],
   "source": [
    "output_LM = model_LM(**inputs)\n",
    "print(output_LM[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802b9bad-6646-4e2d-bcd2-c74e0518a8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_classfication = model_classfication(**inputs)\n",
    "print(output_classfication[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e428048b-0893-46a8-81c2-952c13badfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2769, 2157, 4638, 2207, 4318, 3221, 7946, 5682, 4638,  102, 2769,\n",
      "         2157, 4638, 2207, 4318, 3221,  784,  720, 7582, 5682, 8043,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "torch.Size([1, 23])\n",
      "torch.Size([1, 23])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    input_Q = \"我家的小狗是黑色的\"\n",
    "    input_A = \"我家的小狗是什么颜色？\"\n",
    "    inputs_QA = tokenizer(input_Q, input_A, return_tensors='pt')\n",
    "    print(inputs_QA)\n",
    "    output_QA = model_QA(**inputs_QA)\n",
    "print(output_QA[0].shape)\n",
    "print(output_QA[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a332c9a4-9371-4610-8277-0ac3367fd8f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.519049346446991}]\n",
      "[{'label': 'LABEL_0', 'score': 0.5215342044830322}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import os\n",
    "# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "# 设置代理\n",
    "os.environ['http_proxy'] = 'http://localhost:15236'\n",
    "os.environ['https_proxy'] ='http://localhost:15236'\n",
    "classifier = pipeline('sentiment-analysis',model='google-bert/bert-base-uncased')\n",
    "print(classifier('We feel very very bad!'))\n",
    "print(classifier('We feel very happy!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f24e218-bfa1-4c89-b5f6-b4e6c983ba45",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c92455e04e04605a391109aed932388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\mykerasenv\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\00036777\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9158da0c944eb08dbe79ec5530e0ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd26660fa794cccb41b87f5117a12e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a1961c29c6c4b8381fbd6774cc7d819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9991129040718079}]\n",
      "[{'label': 'POSITIVE', 'score': 0.9998656511306763}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# 设置代理\n",
    "os.environ['http_proxy'] = 'http://localhost:15236'\n",
    "os.environ['https_proxy'] ='http://localhost:15236'\n",
    "nlp = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "print(nlp(\"I hate you\"))\n",
    "print(nlp(\"I love you\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a67815f-a1db-4336-8aa2-2d2312570297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5184928-1d0c-4336-819c-d8b32cb435c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddaf464d061243e9b4a3e537f2da7f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1104 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 设置代理\n",
    "import os\n",
    "os.environ['http_proxy'] = 'http://localhost:15236'\n",
    "os.environ['https_proxy'] ='http://localhost:15236'\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"nyu-mll/glue\", \"ax\")\n",
    "ds.save_to_disk(\"./GLUE/ax\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3655e44b-607e-417a-8be6-e5dbca12d592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import os\n",
    "# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "# 设置代理\n",
    "os.environ['http_proxy'] = 'http://localhost:15236'\n",
    "os.environ['https_proxy'] ='http://localhost:15236'\n",
    "classifier = pipeline('text-classification',model='colourrain/my_first_finetune')\n",
    "print(classifier('the stock market is down'))\n",
    "print(classifier('the stock market is raising'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da1644b-fbeb-4981-bbf5-e22c42142b72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
